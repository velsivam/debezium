Insert the new record in oracle:
--------------------------------

SQL> insert into customers values (null, 'Ravi', 'S', 'ravi@kloud.com');

Getting Kafka logs:
------------------

{"schema":{"type":"struct","fields":[{"type":"int16","optional":false,"field":"ID"}],"optional":false,"name":"server1.DEBEZIUM.CUSTOMERS.Key"},"payload":{"ID":1025}}	{"schema":{"type":"struct","fields":[{"type":"int16","optional":false,"field":"ID"},{"type":"string","optional":false,"field":"FIRST_NAME"},{"type":"string","optional":false,"field":"LAST_NAME"},{"type":"string","optional":false,"field":"EMAIL"}],"optional":true,"name":"server1.DEBEZIUM.CUSTOMERS.Value"},"payload":{"ID":1025,"FIRST_NAME":"Ravi","LAST_NAME":"S","EMAIL":"ravi@kloud.com"}}

Getting Sync connector logs:
----------------------------

[2019-11-07 13:28:06,018] INFO Closing BufferedRecords with updatePreparedStatement: null deletePreparedStatement: null (io.confluent.connect.jdbc.sink.BufferedRecords:242)
[2019-11-07 13:28:06,019] INFO Closing BufferedRecords with updatePreparedStatement: INSERT INTO "server1"."DEBEZIUM"."CUSTOMERS"("ID","FIRST_NAME","LAST_NAME","EMAIL") VALUES(1025,'Ravi','S','ravi@kloud.com') deletePreparedStatement: null (io.confluent.connect.jdbc.sink.BufferedRecords:242)

Postgres output:
----------------

server1=# select * from "DEBEZIUM"."CUSTOMERS";
  ID  | LAST_NAME |     EMAIL      | FIRST_NAME 
------+-----------+----------------+------------
 1025 | S         | ravi@kloud.com | Ravi


Update the record in oracle:
----------------------------

SQL> update customers set email='ravi@kloudone.com' where id=1025;

Getting Kafka logs:
-------------------

{"schema":{"type":"struct","fields":[{"type":"int16","optional":false,"field":"ID"}],"optional":false,"name":"server1.DEBEZIUM.CUSTOMERS.Key"},"payload":{"ID":1025}}	{"schema":{"type":"struct","fields":[{"type":"int16","optional":false,"field":"ID"},{"type":"string","optional":false,"field":"FIRST_NAME"},{"type":"string","optional":false,"field":"LAST_NAME"},{"type":"string","optional":false,"field":"EMAIL"}],"optional":true,"name":"server1.DEBEZIUM.CUSTOMERS.Value"},"payload":{"ID":1025,"FIRST_NAME":"Ravi","LAST_NAME":"S","EMAIL":"ravi@kloudone.com"}}

Getting Sync connector logs:
----------------------------

[2019-11-07 13:29:22,510] INFO Closing BufferedRecords with updatePreparedStatement: null deletePreparedStatement: null (io.confluent.connect.jdbc.sink.BufferedRecords:242)
[2019-11-07 13:29:22,511] INFO Closing BufferedRecords with updatePreparedStatement: INSERT INTO "server1"."DEBEZIUM"."CUSTOMERS"("ID","FIRST_NAME","LAST_NAME","EMAIL") VALUES(1025,'Ravi','S','ravi@kloudone.com') deletePreparedStatement: null (io.confluent.connect.jdbc.sink.BufferedRecords:242)

Postgres output:
----------------

server1=# select * from "DEBEZIUM"."CUSTOMERS";
  ID  | LAST_NAME |       EMAIL       | FIRST_NAME 
------+-----------+-------------------+------------
 1025 | S         | ravi@kloud.com    | Ravi
 1025 | S         | ravi@kloudone.com | Ravi


Delete the record in oracle:
----------------------------

SQL> delete customers where id=1025;

Getting Kafga logs:
-------------------

{"schema":{"type":"struct","fields":[{"type":"int16","optional":false,"field":"ID"}],"optional":false,"name":"server1.DEBEZIUM.CUSTOMERS.Key"},"payload":{"ID":1025}}	null


Getting Sync connector logs:
----------------------------

[2019-11-07 13:30:43,527] ERROR WorkerSinkTask{id=test-sink-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. (org.apache.kafka.connect.runtime.WorkerSinkTask:558)
org.apache.kafka.connect.errors.ConnectException: No fields found using key and value schemas for table: CUSTOMERS

Postgres output:
----------------

server1=# select * from "DEBEZIUM"."CUSTOMERS";
  ID  | LAST_NAME |       EMAIL       | FIRST_NAME 
------+-----------+-------------------+------------
 1025 | S         | ravi@kloud.com    | Ravi
 1025 | S         | ravi@kloudone.com | Ravi

